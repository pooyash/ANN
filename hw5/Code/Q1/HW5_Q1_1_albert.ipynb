{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edLoRrm9_OJX",
        "outputId": "66aaf09f-8d1f-491d-d2e2-395e1dac377f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU:  Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os.path as path\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "sys.path.append(path.join('drive','My Drive','Colab Notebooks'))\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "clear_output()\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('GPU: ',torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q-Y7c5l_QLp",
        "outputId": "04c85f82-377d-4101-a91c-7b369f5e4ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRX_Jorw_SKo",
        "outputId": "2032b565-4843-4bdf-df63-515311138ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "3b0e54c1b2c14130867f1c5b5f737c00",
            "a79729dfa9134db7a36eb64bdc3b9c3a",
            "d7571944c6ba408faf645ab97680be31",
            "9c7f3693d5274040b80a90db5d92d1f7",
            "06bba0046e204983a4e5006a5e52f12f",
            "582845f6045f4946a45219095c72f9a6",
            "13e38a8134974592bf58b8564ac61f70",
            "5011735fa2de4c559da2e2519f0e5da9",
            "02f8c2aa049b4820b697c8e3f3536bd7",
            "71f48d8377be419b90c373871e08f3ab",
            "b4b748003a034313a58cc17660f33f13"
          ]
        },
        "id": "6GqvALNQ_cGw",
        "outputId": "b6347a31-4a42-4101-a3b2-a0fb7c8b1e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Evaluate the last checkpoint\n",
            "Loading features from cached file /content/drive/MyDrive/HW_5/Dataset/cached_test\n",
            "\n",
            "Evaluation:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b0e54c1b2c14130867f1c5b5f737c00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/1021 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluation done in total 213.4106203199999 secs (0.026137246824249834 sec per example)\n",
            "{'exact': 50.44988752811797, 'f1': 64.65321757161851, 'total': 8002, 'HasAns_exact': 66.22864651773982, 'HasAns_f1': 84.89734674902944, 'HasAns_total': 6088, 'NoAns_exact': 0.2612330198537095, 'NoAns_f1': 0.2612330198537095, 'NoAns_total': 1914, 'best_exact': 50.39990002499375, 'best_exact_thresh': 0.0, 'best_f1': 64.60323006849427, 'best_f1_thresh': 0.0}\n",
            "\n",
            "Result:\n",
            "\texact_match: 50.44988752811797\n",
            "\tf1: 64.65321757161851\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import timeit\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install transformers\n",
        "!pip install -q sentencepiece\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    squad_convert_examples_to_features,\n",
        ")\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "\"\"\"Parameters\"\"\"\n",
        "\n",
        "\n",
        "model_name_or_path = 'm3hrdadfi/albert-fa-base-v2'\n",
        "#model_name_or_path = 'HooshvareLab/bert-base-parsbert-uncased'\n",
        "output_dir = '/content/drive/MyDrive/HW_5/Dataset'\n",
        "dataset_name = ''\n",
        "version_2 = False\n",
        "null_score_diff_threshold = 0\n",
        "max_seq_length = 512\n",
        "max_query_length = 64\n",
        "doc_stride = 128\n",
        "do_lowercase = False\n",
        "per_gpu_train_batch_size = 12\n",
        "per_gpu_eval_batch_size = 8\n",
        "learning_rate = 3e-5\n",
        "gradient_accumulation_steps = 1\n",
        "weight_decay = 0\n",
        "max_grad_norm = 1\n",
        "num_train_epochs = 1\n",
        "warmup_steps = 0\n",
        "n_best_size = 20\n",
        "max_answer_length = 30\n",
        "seed = 42\n",
        "threads = 10\n",
        "\n",
        "\"\"\"Parameters\"\"\"\n",
        "\n",
        "\n",
        "def set_seed():\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "\n",
        "def train(train_dataset, model, tokenizer, start_epoch):\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=per_gpu_train_batch_size)\n",
        "\n",
        "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
        "\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    if os.path.isfile(os.path.join(output_dir, \"optimizer.pt\")) and os.path.isfile(\n",
        "        os.path.join(output_dir, \"scheduler.pt\")\n",
        "    ):\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(output_dir, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(output_dir, \"scheduler.pt\")))\n",
        "\n",
        "    print(\"\\nTraining:\")\n",
        "\n",
        "    global_step = 1\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    set_seed()\n",
        "\n",
        "    for epoch_idx in range(num_train_epochs):\n",
        "        if epoch_idx < start_epoch:\n",
        "            continue\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration in epoch {}\".format(epoch_idx+1))\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "            if n_gpu > 1:\n",
        "                loss = loss.mean()\n",
        "            if gradient_accumulation_steps > 1:\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "        checkpoint_output_dir = os.path.join(output_dir, 'checkpoint_{}'.format(epoch_idx+1))\n",
        "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "        model_to_save.save_pretrained(checkpoint_output_dir)\n",
        "        tokenizer.save_pretrained(checkpoint_output_dir)\n",
        "        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(model, tokenizer):\n",
        "    dataset, examples, features = load_and_cache_examples(tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=per_gpu_eval_batch_size)\n",
        "\n",
        "    print(\"\\nEvaluation:\")\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "            feature_indices = batch[3]\n",
        "            outputs = model(**inputs, return_dict=False)\n",
        "\n",
        "            for i, feature_index in enumerate(feature_indices):\n",
        "                eval_feature = features[feature_index.item()]\n",
        "                unique_id = int(eval_feature.unique_id)\n",
        "                output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "                all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    print(\"  Evaluation done in total {} secs ({} sec per example)\".format(evalTime, evalTime / len(dataset)))\n",
        "\n",
        "    output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
        "    output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
        "\n",
        "    if version_2:\n",
        "        output_null_log_odds_file = os.path.join(output_dir, \"null_odds.json\")\n",
        "    else:\n",
        "        output_null_log_odds_file = None\n",
        "    \n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        n_best_size,\n",
        "        max_answer_length,\n",
        "        do_lowercase,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        False,\n",
        "        version_2,\n",
        "        null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    if output_null_log_odds_file is not None:\n",
        "        filename = os.path.join(output_dir, 'null_odds.json')\n",
        "        null_odds = json.load(open(filename, 'rb'))\n",
        "    else:\n",
        "        null_odds = None\n",
        "    results = squad_evaluate(examples, predictions, no_answer_probs=null_odds, no_answer_probability_threshold=null_score_diff_threshold)\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(tokenizer, evaluate=False, output_examples=False):\n",
        "    input_dir = os.path.join('/content/drive/MyDrive/HW_5/Dataset/', dataset_name)\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}\".format(\"test\" if evaluate else \"train\"),\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file):\n",
        "        print(\"Loading features from cached file {}\".format(cached_features_file))\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        print(\"Creating features from dataset file at {}\".format(input_dir))\n",
        "        processor = SquadV2Processor() if version_2 else SquadV1Processor()\n",
        "        if evaluate:\n",
        "            examples = processor.get_dev_examples(input_dir, filename='Test.json')\n",
        "        else:\n",
        "            examples = processor.get_train_examples(input_dir, filename='Train.json')\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=threads,\n",
        "        )\n",
        "        print(\"Saving features into cached file {}\".format(cached_features_file))\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main():\n",
        "    set_seed()\n",
        "\n",
        "    global_step = \"\"\n",
        "\n",
        "    model_file = os.path.join(output_dir,'pytorch_model.bin')\n",
        "    do_train = not os.path.exists(model_file)\n",
        "    if do_train:\n",
        "        checkpoints_dir = filter(lambda x:x.startswith('checkpoint_'), os.listdir(output_dir))\n",
        "        checkpoint = max(map(lambda x:int(x[x.find('_')+1:]), checkpoints_dir), default=0)\n",
        "        if checkpoint == 0:\n",
        "            config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, do_lower_case=do_lowercase, use_fast=False)\n",
        "            train_dataset = load_and_cache_examples(tokenizer, evaluate=False, output_examples=False)\n",
        "            model = AutoModel.from_pretrained(model_name_or_path, config=config)\n",
        "        else:\n",
        "            checkpoint_output_dir = os.path.join(output_dir, 'checkpoint_{}'.format(checkpoint))\n",
        "            config = AutoConfig.from_pretrained(checkpoint_output_dir)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(checkpoint_output_dir, do_lower_case=do_lowercase, use_fast=False)\n",
        "            train_dataset = load_and_cache_examples(tokenizer, evaluate=False, output_examples=False)\n",
        "            model = AutoModel.from_pretrained(checkpoint_output_dir, config=config)\n",
        "        model.to(device)\n",
        "        global_step, tr_loss = train(train_dataset, model, tokenizer, checkpoint)\n",
        "        print(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))\n",
        "        print(\"Saving model checkpoint to {}\".format(output_dir))\n",
        "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    result = {}\n",
        "    print(\"Evaluate the last checkpoint\")\n",
        "    config = AutoConfig.from_pretrained(output_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(output_dir, do_lower_case=do_lowercase, use_fast=False)\n",
        "    model = AutoModel.from_pretrained(output_dir, config=config)\n",
        "    model.to(device)\n",
        "    eval_result = evaluate(model, tokenizer)\n",
        "    result = dict((k, v) for k, v in eval_result.items())\n",
        "\n",
        "    print(result)\n",
        "    print(\"\\nResult:\\n\\texact_match: {}\\n\\tf1: {}\".format(result['exact'], result['f1']))\n",
        "    return result\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02f8c2aa049b4820b697c8e3f3536bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06bba0046e204983a4e5006a5e52f12f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13e38a8134974592bf58b8564ac61f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b0e54c1b2c14130867f1c5b5f737c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a79729dfa9134db7a36eb64bdc3b9c3a",
              "IPY_MODEL_d7571944c6ba408faf645ab97680be31",
              "IPY_MODEL_9c7f3693d5274040b80a90db5d92d1f7"
            ],
            "layout": "IPY_MODEL_06bba0046e204983a4e5006a5e52f12f"
          }
        },
        "5011735fa2de4c559da2e2519f0e5da9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "582845f6045f4946a45219095c72f9a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f48d8377be419b90c373871e08f3ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7f3693d5274040b80a90db5d92d1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71f48d8377be419b90c373871e08f3ab",
            "placeholder": "​",
            "style": "IPY_MODEL_b4b748003a034313a58cc17660f33f13",
            "value": " 1021/1021 [03:33&lt;00:00,  5.28it/s]"
          }
        },
        "a79729dfa9134db7a36eb64bdc3b9c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_582845f6045f4946a45219095c72f9a6",
            "placeholder": "​",
            "style": "IPY_MODEL_13e38a8134974592bf58b8564ac61f70",
            "value": "Evaluating: 100%"
          }
        },
        "b4b748003a034313a58cc17660f33f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7571944c6ba408faf645ab97680be31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5011735fa2de4c559da2e2519f0e5da9",
            "max": 1021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02f8c2aa049b4820b697c8e3f3536bd7",
            "value": 1021
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
